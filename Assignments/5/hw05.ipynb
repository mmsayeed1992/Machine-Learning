{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Murt Sayeed\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW05 Code\n",
    "\n",
    "\n",
    "You will complete the following notebook, as described in the PDF for Homework 05 (included in the download with the starter code).  You will submit:\n",
    "1. This notebook file, along with your COLLABORATORS.txt file and the two tree images (PDFs generated using `graphviz` within the code), to the Gradescope link for code.\n",
    "2. A PDF of this notebook and all of its output, once it is completed, to the Gradescope link for the PDF.\n",
    "\n",
    "\n",
    "Please report any questions to the [class Piazza page](https://piazza.com/tufts/spring2021/comp135).\n",
    "\n",
    "### Import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.tree\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "You should start by computing the two heuristic values for the toy data described in the assignment handout. You should then load the two versions of the abalone data, compute the two heuristic values on features (for the simplified data), and then build decision trees for each set of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Compute both heuristics for toy data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Compute the counting-based heuristic, and order the features by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature A: 6/8\n",
      "Feature B: 6/8\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "#d = dict() dictionary to store the number of labels in each attribute\n",
    "d_A = {'True': {'circle':2, 'cross':0}, 'False': {'circle':2, 'cross':4} }\n",
    "d_B = {'True': {'circle':3, 'cross':1}, 'False': {'circle':1, 'cross':3} }\n",
    "\n",
    "def guess_dict(d):\n",
    "    for key in d:\n",
    "        if d[key]['circle'] >= d[key]['cross']:\n",
    "            d[key]['guess'] = 'circle'\n",
    "        else:\n",
    "            d[key]['guess'] = 'cross'\n",
    "\n",
    "guess_dict(d_A)\n",
    "guess_dict(d_B)\n",
    "\n",
    "def compute_score(d):\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    for key in d:\n",
    "        num_correct += d[key][d[key]['guess']]\n",
    "        num_total += d[key]['circle']\n",
    "        num_total += d[key]['cross']\n",
    "    score = num_correct/num_total\n",
    "    return score, num_correct, num_total\n",
    "\n",
    "score_A, corr_A, total_A = compute_score(d_A)\n",
    "score_B, corr_B, total_B = compute_score(d_B)\n",
    "\n",
    "if score_A >= score_B:\n",
    "    print('Feature A: ', corr_A, '/', total_A, sep='')\n",
    "    print('Feature B: ', corr_B, '/', total_B, sep='')\n",
    "else:\n",
    "    print('Feature B: ', corr_B, '/', total_B, sep='')\n",
    "    print('Feature A: ', corr_A, '/', total_A, sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Compute the information-theoretic heuristic, and order the features by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature A: 0.311\n",
      "Feature B: 0.189\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "d_All = {'circle': 4, 'cross': 4}\n",
    "d_A = {'True': {'circle':2, 'cross':0}, 'False': {'circle':2, 'cross':4} }\n",
    "d_B = {'True': {'circle':3, 'cross':1}, 'False': {'circle':1, 'cross':3} }\n",
    "\n",
    "def Entropy(d):\n",
    "    circle = d['circle']\n",
    "    cross = d['cross']\n",
    "    total = sum(d.values())\n",
    "    if circle == 0:\n",
    "        return -1*(cross/total*np.log2(cross/total))\n",
    "    elif cross == 0:\n",
    "        return -1*(circle/total*np.log2(circle/total))\n",
    "    else:\n",
    "        return -1*(circle/total*np.log2(circle/total) + cross/total*np.log2(cross/total))\n",
    "\n",
    "def Gain(d, entropy_fullset):\n",
    "    entr_Tr  = Entropy(d['True'])\n",
    "    count_Tr = sum(d['True'].values())\n",
    "    entr_Fa  = Entropy(d['False'])\n",
    "    count_Fa = sum(d['False'].values())\n",
    "    total = count_Tr + count_Fa\n",
    "    \n",
    "    return entropy_fullset - (count_Tr / total * entr_Tr + count_Fa / total * entr_Fa)\n",
    "\n",
    "\n",
    "ent_All = Entropy(d_All)\n",
    "\n",
    "gain_A = Gain(d_A, ent_All)\n",
    "gain_B = Gain(d_B, ent_All)\n",
    "\n",
    "if gain_A >= gain_B:\n",
    "    print('Feature A: %.3f' % gain_A)\n",
    "    print('Feature B: %.3f' % gain_B)\n",
    "else:\n",
    "    print('Feature B: %.3f' % gain_B)\n",
    "    print('Feature A: %.3f' % gain_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Discussion of results.\n",
    "\n",
    "*TODO* Discuss the results: if we built a tree using each of these heuristics, what would\n",
    "happen? What does this mean?\n",
    "\n",
    "We have to look at two methods and see what can give us better results and information. The counting-based is simple condition and information-theoretic can provide better and accurate information. The counting-based divides the data into two groups that are identical and didn't find a difference b/w the scores of each feature (A & B). These were a same/tie because of the nature of counting-based method. Our main goal should be to have better and more accurate feature out of all and reduce complexity as well. In information-theoretic method, we concluded Feature A having 0.311 of information gain and Feature B having 0.189, where Feature A is more important than Feature B. Thus, the second method of information-theoretic is a better option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Compute both heuristics for simplified abalone data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Compute the counting-based heuristic, and order the features by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "height_mm: 2316/3176\n",
      "diam_mm: 2266/3176\n",
      "length_mm: 2230/3176\n",
      "is_male: 1864/3176\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "X_train_simple = pd.read_csv('data_abalone/small_binary_x_train.csv')\n",
    "X_test_simple = pd.read_csv('data_abalone/small_binary_x_test.csv')\n",
    "y_train_simple = pd.read_csv('data_abalone/3class_y_train.csv')\n",
    "y_test_simple = pd.read_csv('data_abalone/3class_y_test.csv')\n",
    "\n",
    "features = []\n",
    "\n",
    "for c in list(X_train_simple.columns):\n",
    "    var0_all_ys = y_train_simple[X_train_simple[c]==0]\n",
    "    var1_all_ys = y_train_simple[X_train_simple[c]==1]\n",
    "    \n",
    "    var0_y0 = (var0_all_ys == 0).sum()['rings']\n",
    "    var0_y1 = (var0_all_ys == 1).sum()['rings']\n",
    "    var0_y2 = (var0_all_ys == 2).sum()['rings']\n",
    "    \n",
    "    var1_y0 = (var1_all_ys == 0).sum()['rings']\n",
    "    var1_y1 = (var1_all_ys == 1).sum()['rings']\n",
    "    var1_y2 = (var1_all_ys == 2).sum()['rings']\n",
    "    \n",
    "    numerator_c = max(var0_y0, var0_y1, var0_y2) + max(var1_y0, var1_y1, var1_y2)\n",
    "    denominator_c = var0_y0 + var0_y1 + var0_y2 + var1_y0 + var1_y1 + var1_y2\n",
    "    score_c = numerator_c / denominator_c\n",
    "    \n",
    "    features.append((c,score_c, numerator_c, denominator_c))\n",
    "\n",
    "features.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for f in features:\n",
    "    print(f[0], ': ', f[2], '/', f[3], sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Compute the information-theoretic heuristic, and order the features by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "height_mm: 0.17302867291002477\n",
      "diam_mm: 0.1500706886802703\n",
      "length_mm: 0.13543816377043694\n",
      "is_male: 0.024516482271752293\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "features_gain = []\n",
    "\n",
    "def Entropy_aba(*args):\n",
    "    total = 0\n",
    "    for arg in args:\n",
    "        total += arg\n",
    "    entro = 0\n",
    "    for arg in args:\n",
    "        p_cat = arg / total\n",
    "        entro += (p_cat * np.log2(p_cat))\n",
    "    return -entro\n",
    "\n",
    "entro_fullset = Entropy_aba((y_train_simple == 0).sum()['rings'],(y_train_simple == 1).sum()['rings'],(y_train_simple == 2).sum()['rings'])\n",
    "\n",
    "for c in list(X_train_simple.columns):\n",
    "    var0_all_ys = y_train_simple[X_train_simple[c]==0]\n",
    "    var1_all_ys = y_train_simple[X_train_simple[c]==1]\n",
    "    \n",
    "    var0_y0 = (var0_all_ys == 0).sum()['rings']\n",
    "    var0_y1 = (var0_all_ys == 1).sum()['rings']\n",
    "    var0_y2 = (var0_all_ys == 2).sum()['rings']\n",
    "    \n",
    "    var1_y0 = (var1_all_ys == 0).sum()['rings']\n",
    "    var1_y1 = (var1_all_ys == 1).sum()['rings']\n",
    "    var1_y2 = (var1_all_ys == 2).sum()['rings']\n",
    "    \n",
    "    entro_0 = Entropy_aba(var0_y0, var0_y1, var0_y2)\n",
    "    entro_1 = Entropy_aba(var1_y0, var1_y1, var1_y2)\n",
    "    \n",
    "    ratio_0 = var0_all_ys.size / y_train_simple.size \n",
    "    ratio_1 = var1_all_ys.size / y_train_simple.size\n",
    "    \n",
    "    gain = entro_fullset - (ratio_0 * entro_0 + ratio_1 * entro_1)\n",
    "    \n",
    "    features_gain.append((c, gain))\n",
    "    \n",
    "features_gain.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for f in features_gain:\n",
    "    print(f[0], ': ', f[1], sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Generate decision trees for full- and restricted-feature data\n",
    "\n",
    "#### (a) Print accuracy values and generate tree images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL DATA-SET\n",
      "Accuracy Training Set: 1.0000\n",
      "Accuracy Testing Set: 0.1840\n",
      "\n",
      "SIMPLIFIED DATA-SET\n",
      "Accuracy Training Set: 0.7327\n",
      "Accuracy Testing Set: 0.7220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "X_train_full = pd.read_csv('data_abalone/x_train.csv')\n",
    "X_test_full = pd.read_csv('data_abalone/x_test.csv')\n",
    "y_train_full = pd.read_csv('data_abalone/y_train.csv')\n",
    "y_test_full = pd.read_csv('data_abalone/y_test.csv')\n",
    "\n",
    "\n",
    "dec_full = sklearn.tree.DecisionTreeClassifier(criterion='entropy')\n",
    "dec_full.fit(X_train_full, y_train_full)\n",
    "train_full_score = dec_full.score(X_train_full, y_train_full)\n",
    "test_full_score = dec_full.score(X_test_full, y_test_full)\n",
    "\n",
    "print('FULL DATA-SET')\n",
    "print('Accuracy Training Set: %.4f' % train_full_score)\n",
    "print('Accuracy Testing Set: %.4f' % test_full_score)\n",
    "print('')\n",
    "\n",
    "dec_simple = sklearn.tree.DecisionTreeClassifier(criterion='entropy')\n",
    "dec_simple.fit(X_train_simple, y_train_simple)\n",
    "train_simple_score = dec_simple.score(X_train_simple, y_train_simple)\n",
    "test_simple_score = dec_simple.score(X_test_simple, y_test_simple)\n",
    "\n",
    "print('SIMPLIFIED DATA-SET')\n",
    "print('Accuracy Training Set: %.4f' % train_simple_score)\n",
    "print('Accuracy Testing Set: %.4f' % test_simple_score)\n",
    "print('')\n",
    "\n",
    "dec_full_data = sklearn.tree.export_graphviz(dec_full, out_file=None) \n",
    "graph = graphviz.Source(dec_full_data) \n",
    "graph.render(\"full\");\n",
    "\n",
    "dec_simple_data = sklearn.tree.export_graphviz(dec_simple, out_file=None) \n",
    "graph_simple = graphviz.Source(dec_simple_data) \n",
    "graph_simple.render(\"simple\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Discuss the results seen for the two trees\n",
    "\n",
    "If we analyze the two decision trees, the simplified data-set model is smaller compared to full data-set, where there is much more depth and large number of leaves. It seems the training accuracy (100%) is better in full data-set for obviously reasons since we have more features. But testing accuracy (18.4%) is very poor. This issue is likely due to overfitting and we shouldn't generalizd this method to other cases and examples. We have tried to use full data-set which contains all the feature and every single details, and the method will conclude in overfitting to fit all our inputs from the full data-set. When we have large tree in full data-set with lots of branches and leaves, there has to be overfitting problem. To fix the problem, we should utilize smaller data-set b/w our true condition vs. the prediction. The simplified model has the training accuracy (73.3%) around the same level as testing accuracy (72.2%), which reflects a better model and doesn't cause the overfitting issue. The tree level in terms of leaves, branches and depth is much better in simplified model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
